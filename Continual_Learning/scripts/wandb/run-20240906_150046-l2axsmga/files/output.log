
[34m[1mwandb[39m[22m: [33mWARNING[39m No relevant files were detected in the specified directory. No code will be logged to your run.
Experiment dir: ./exp/Test_los_south_baseline
------------------------------------
Task: 1, Epoch: 1
Train: epoch: 1, loss = 2.1584853649139406
Train: epoch: 1, loss = 2.128408248722553
Train: epoch: 1, loss = 2.115709305802981
Train: epoch: 1, loss = 2.108860559463501
Train: epoch: 1, loss = 2.097797573685646
Train: epoch: 1, loss = 2.087023118833701
Train: epoch: 1, loss = 2.0813686813626973
Train: epoch: 1, loss = 2.076973155811429
Train: epoch: 1, loss = 2.0744438769419986
Train: epoch: 1, loss = 2.0731214446425437
Train: epoch: 1, loss = 2.0715068793838673
Train: epoch: 1, loss = 2.068788267026345
Train: epoch: 1, loss = 2.0661149025880374
Train: epoch: 1, loss = 2.0657258238536973
Train: epoch: 1, loss = 2.063852287451426
Train: epoch: 1, loss = 2.0629588269814847
Train: epoch: 1, loss = 2.0615591479399624
Train: epoch: 1, loss = 2.06123163200087
Train: epoch: 1, loss = 2.0608949196024944
Train: epoch: 1, loss = 2.060317780315876
Train: epoch: 1, loss = 2.059400386469705
Train: epoch: 1, loss = 2.0584576856277206
Train: epoch: 1, loss = 2.057638255383657
Train: epoch: 1, loss = 2.0568093721320233
Train: epoch: 1, loss = 2.0562929110765458
Train: epoch: 1, loss = 2.055219519986556
Train: epoch: 1, loss = 2.05549523960661
Train: epoch: 1, loss = 2.0550308858283928
Train: epoch: 1, loss = 2.054652535483755
Train: epoch: 1, loss = 2.05409089332819
Train: epoch: 1, loss = 2.0530331287653216
Train: epoch: 1, loss = 2.052831506263465
Train: epoch: 1, loss = 2.051228678154223
Train: epoch: 1, loss = 2.0505554974780362
Train: epoch: 1, loss = 2.0499765971047537
Train: epoch: 1, loss = 2.049617418696483
Train: epoch: 1, loss = 2.0489883276739635
Train: epoch: 1, loss = 2.0486143433577135
Train: epoch: 1, loss = 2.0475280349529705
Train: epoch: 1, loss = 2.0468642208576204
Train: epoch: 1, loss = 2.046844503646944
Train: epoch: 1, loss = 2.0461547591288887
Train: epoch: 1, loss = 2.0460941695473913
Train:  Epoch 1, Loss=2.0462366937909806, Cohen Kappa=0.3734431736977528, MAD=0.7193080278732507
-------------
Eval task: 1
Eval:  Epoch 1, Loss=2.0324214018624405, Cohen Kappa=0.43226060623879103, MAD=0.7318024004211913
Eval task: 2
Eval:  Epoch 1, Loss=1.925169509032677, Cohen Kappa=0.0015985417291888293, MAD=0.7469613290024189
-------------------------
Testing task: 1
-------------------------
Test:  Epoch 1, Loss=2.060100023088784, Cohen Kappa=0.3382691785296619, MAD=0.7310947856449946
-------------------------
Testing task: 2
-------------------------
Test:  Epoch 1, Loss=1.9179954775448502, Cohen Kappa=0.0026661916513568418, MAD=0.7478197955039549
------------------------------------
Task: 2, Epoch: 1
Train: epoch: 1, loss = 1.9197509843111038
Train: epoch: 1, loss = 1.9218831777572631
Train: epoch: 1, loss = 1.9166918867826461
Train: epoch: 1, loss = 1.917035784870386
Train: epoch: 1, loss = 1.9155847096443177
Train: epoch: 1, loss = 1.9143261594573657
Train: epoch: 1, loss = 1.912029685974121
Train: epoch: 1, loss = 1.913615656197071
Train: epoch: 1, loss = 1.9151015428039764
Train: epoch: 1, loss = 1.9145603933930397
Train: epoch: 1, loss = 1.913801553249359
Train: epoch: 1, loss = 1.9135756104687849
Train: epoch: 1, loss = 1.9132950415519567
Train: epoch: 1, loss = 1.9136559699262892
Train: epoch: 1, loss = 1.9132684171597163
Train: epoch: 1, loss = 1.9123529780656099
Train: epoch: 1, loss = 1.9120913234528374
Train: epoch: 1, loss = 1.911954746776157
Train: epoch: 1, loss = 1.9114116067949094
Train: epoch: 1, loss = 1.9112928011715413
Train: epoch: 1, loss = 1.911873898931912
Train: epoch: 1, loss = 1.9120338521762328
Train: epoch: 1, loss = 1.911756903451422
Train: epoch: 1, loss = 1.911030078108112
Train: epoch: 1, loss = 1.9109971465826034
Train: epoch: 1, loss = 1.9098902971698688
Train: epoch: 1, loss = 1.9098850742755114
Train: epoch: 1, loss = 1.9095549569172519
Train: epoch: 1, loss = 1.9092394663958714
Train: epoch: 1, loss = 1.9090995252331098
Train: epoch: 1, loss = 1.9089761537698007
Train: epoch: 1, loss = 1.9089397505111991
Train: epoch: 1, loss = 1.9086251523819837
Train: epoch: 1, loss = 1.9084290348256336
Train: epoch: 1, loss = 1.908296442747116
Train: epoch: 1, loss = 1.9086054355237219
Train: epoch: 1, loss = 1.9086935255817465
Train: epoch: 1, loss = 1.908875570109016
Train: epoch: 1, loss = 1.9086796503800612
Train: epoch: 1, loss = 1.9084199133813382
Train: epoch: 1, loss = 1.9080628679438336
Train: epoch: 1, loss = 1.9080261182501204
Train: epoch: 1, loss = 1.9076089531183242
Train:  Epoch 1, Loss=1.9072998039109366, Cohen Kappa=0.08698239428355181, MAD=0.6945774291777803
-------------
Eval task: 1
Eval:  Epoch 1, Loss=2.1713652446352203, Cohen Kappa=0.02252634321634328, MAD=0.6890815273351312
Eval task: 2
Eval:  Epoch 1, Loss=1.9060631394386292, Cohen Kappa=0.16883468281746328, MAD=0.6916730178414351
-------------------------
Testing task: 1
-------------------------
Test:  Epoch 1, Loss=2.117826726929895, Cohen Kappa=0.02283557245724932, MAD=0.6984046188372057
-------------------------
Testing task: 2
-------------------------
Test:  Epoch 1, Loss=1.897177887373957, Cohen Kappa=0.09452653612964623, MAD=0.691119850520239
{'0': {'precision': 0.42410559221952066, 'recall': 0.2996319018404908, 'f1-score': 0.3511647972389992, 'support': 4075}, '1': {'precision': 0.18205019140791154, 'recall': 0.7469458987783595, 'f1-score': 0.292749658002736, 'support': 2865}, '2': {'precision': 1.0, 'recall': 0.0011001100110011, 'f1-score': 0.002197802197802198, 'support': 1818}, '3': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1249}, '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 857}, '5': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 662}, '6': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 569}, '7': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 464}, '8': {'precision': 0.3584905660377358, 'recall': 0.015625, 'f1-score': 0.029944838455476755, 'support': 1216}, '9': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1073}, 'accuracy': 0.22777478448275862, 'macro avg': {'precision': 0.19646463496651678, 'recall': 0.10633029106298515, 'f1-score': 0.06760570958950142, 'support': 14848}, 'weighted avg': {'precision': 0.30332223969424166, 'recall': 0.22777478448275862, 'f1-score': 0.155585455743819, 'support': 14848}}
{'0': {'precision': 0.37489609310058186, 'recall': 0.1015308419630797, 'f1-score': 0.15978742249778566, 'support': 4442}, '1': {'precision': 0.3494130042143287, 'recall': 0.9022541780023319, 'f1-score': 0.5037430834327872, 'support': 5146}, '2': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2540}, '3': {'precision': 0.25, 'recall': 0.0007686395080707148, 'f1-score': 0.0015325670498084292, 'support': 1301}, '4': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 527}, '5': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 305}, '6': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 189}, '7': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 110}, '8': {'precision': 0.14666666666666667, 'recall': 0.0625, 'f1-score': 0.08764940239043825, 'support': 176}, '9': {'precision': 0.08424908424908426, 'recall': 0.20535714285714285, 'f1-score': 0.11948051948051949, 'support': 112}, 'accuracy': 0.34543372844827586, 'macro avg': {'precision': 0.12052248482306613, 'recall': 0.12724108023306252, 'f1-score': 0.0872192994851339, 'support': 14848}, 'weighted avg': {'precision': 0.2575341457441373, 'recall': 0.34543372844827586, 'f1-score': 0.22446387532426068, 'support': 14848}}